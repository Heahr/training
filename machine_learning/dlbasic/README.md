## Round 5-3 Begining to Deep Learning

# 딥러닝의 시작

## 인류의 간절한 염원, 인공지능

인공지능을 만들기 위해 다양한 분야에서 수많은 이론이 탄생하게 되었다. 그 중 인간의 뇌를 모방하자는 이론이 바로 'Deep Learning'이다. 인간의 뇌는 다양한 동작을 하게 되는데, 이러한 다양한 동작은 가령 상상, 생각, 무의식 등이다.  

이를 신기하게 여긴 학자들은 다양한 동작을 가능하게 하는 것이 뇌 속의 '뉴런'이 모여 서로 작용하기 때문이라는 것을 밝혀낸다.  

이러한 뉴런은 미량의 전기신호로 정보를 전달하는데, 여기서 뉴런에 입력된 정보를 `x`, 정보를 구분하기 위한 신호의 가중치(weight)를 `w`라고 할 때, 뉴런의 각기 다른 촉수에서 들어온 '`x`와 `w`의 곱'들의 합을 정보의 집합(I)으로 간주할 수 있다.    


![](https://github.com/rjs1197/ml_study/blob/master/assets/1st_week_1.png)

```
입력된 값(x) * 신호의 가중치(w) 들의 합(sigma Σ) = 정보의 집합(I)
```

이렇게 만들어진 하나의 정보는 다음 뉴런으로 전달되기 위해 방향성을 조절하기 위한 값(bios)과 더해지고, 이 값을 활용해 어떠한 동작(function)을 수행하는 과정을 거친다. 여기서 수행되는 과정은 어떤 값 n에 대해 if n > I + b라면 다음 뉴런으로 전달하는 것을 말한다.  

이제 우리는 다음과 같이 하나의 수식을 통해 이 동작을 정의할 수 있다.  

![](https://github.com/rjs1197/ml_study/blob/master/assets/1st_week_2.png)

```
동작(func){정보의 집합(I) + 바이어스(b)} = 가공된 정보(Y)
```

위 과정을 거쳐 하나의 뉴런에서 또다른 뉴런으로 가공된 정보가 이동여부를 결정하게 되며, 이러한 정보 이동의 집합을 'Neural Network'라고 부르게 되었다. 이 과정을 유심히 본 공학자들은 '인공적인 신경망'을 떠올리게 되었다.  

```
이런 수식을 가졌다면 우리도 만들 수 있겠는데?
```

그렇게 탄생한 것이 'Artificial Neural Network(인공신경망)이다. 이러한 이론이 제기된 초기에는 아래와 같이 하드웨어로 구현할 수밖에 없었고, 그들은 이를 '인공지능'이라고 불렀다.  

당시에는 인공지능, 즉 생각하는 기계를 만들기 위해서는 AND/OR의 연산을 예측해야 한다고 생각했고, 그들의 기계는 이를 당연하게 예측했다. 그러나 여기에도 문제가 있었는데, 완전한 인공지능을 위해서는 XOR 연산까지도 예측해야 한다는 것이었다.    

XOR 연산의 경우에는 신경망 학습을 이용한 예측에서 50%의 예측률을 보였고, 신경망 학습을 이용해 완전하게 XOR을 예측하는 것은 불가능하다고 단정짓는다.  

이 불가능을 단정지어버린 사건은 짧게는 15년, 길게는 20년 딥러닝의 역사를 늦춘 계기가 된다. 이후 1986년 Hinton에 의해 해결되기 전까지 '딥러닝'은 '불가능한 기술'이라는 꼬리표를 달았어야 했다(1974년에 이미 해결 방법이 발견되었지만, 당시 학계 분위기로 인해 사장되었다. 시간이 흘러 결국 Hinton이 이를 재발견하게 되었다).  

![](https://github.com/rjs1197/ml_study/blob/master/assets/1st_week_3.png)

이 논문은 학습된 신경망의 출력이 우리가 원하는 값과는 다를 경우 정보를 반대로 전달(backpropagation)하여 뉴런 각각을 변경시켜 결과를 바꾸는 이론이다. 이로인해 XOR은 물론 더 복잡한 형태의 예측이 가능해졌다.  

또 한편으로는 CNNs이라고 불리는 'Convolutional Neural Networks'이 LeCun에 의해 발표되었다.  

![](https://github.com/rjs1197/ml_study/blob/master/assets/1st_week_4.png)
