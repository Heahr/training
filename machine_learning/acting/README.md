## Deep Learning acting  

##### '밑바닥부터 시작하는 딥러닝' 교재를 활용해 간단한 실습을 진행해보자.  

[뒤로가기](/machine_learning/README.md)

이 문서는 각 실습을 설명하기 위한 문서가 아닌 실습의 주된 목적을 기술하기 위해 작성한다.  

본 실습을 통해 퍼셉트론을 구현하는 간단한 기술부터 CNNs를 사용하는 조금은 난해한 예제까지 다뤄볼 것이다. 이를 위해 필요한 필자의 개발환경은 다음과 같다.  

* [Python3.6](https://www.python.org/downloads/)  
* [Anaconda5.2 - Python 3.6 version](/www.anaconda.com/download/)  
* [PyCharm Community](https://www.jetbrains.com/pycharm/download/)  

#### 학습 내용 정리  

##### Numpy  

변수의 행렬 연산을 위해 Numpy 라이브러리를 사용한다. Numpy를 사용하여 행렬 연산을 하면, 가장 큰 특징이 '브로드캐스트' 기능이다. 브로드캐스트는 가령 다음과 같은 상황에서 발휘된다.  

```
>>> import numpy as np
>>> A = np.array([[1, 2], [3, 4]])
>>> A * 10
array([[ 10, 20],
	   [ 30, 40]])
```
  
이처럼 스칼라 값과 행렬을 곱하면, 스칼라 값은 2x2 행렬로 확대된 후 연산이 이루어진다. 이 기능을 브로드캐스트라고 한다.  

그 외에도 인덱스를 배열로 지정해 한 번에 여러 원소에 접근할 수도 있고, 이 기법을 응용해 특정 조건을 만족하는 원소를 얻을 수도 있다.  

```
>>> X = np.array([[51, 55], [14, 19], [0, 4]])
>>> X = X.flatten()	# X를 1차원 배열로 변환(평탄화)
>>> print(X)
[51 55 14 19 0 4]
>>> X[np.array([0, 2, 4])] # 인덱스가 0, 2, 4인 원소 얻기
array([51, 14, 0])
>>> X > 15
array([ True, True, False, True, False, False], dtype=bool)
>>> X[X>15]
array([51, 55, 19])
```
  
##### Matplotlib

Matplotlib 라이브러리는 이미지를 표시하거나 Plot chart를 그리기 위한 라이브러리이다. 이는 실습을 직접 따라해가며 눈으로 익히는 게 이해하기 쉬울 것이다.  

##### Perceptron  

Perceptron, 즉 퍼셉트론은 하나의 뉴런 혹은 노드로 취급되어 다수의 신호를 입력으로 받아 하나의 신호를 출력한다. 가령 논리연산자와 같은 기능을 수행한다.  

그러나 단순히 하나의 퍼셉트론 이론만 가지고는 XOR을 표현할 수 없다. 그 이유는 퍼셉트론은 여러 입력으로부터 하나의 출력을 받는데, 출력을 정하는 기준에 해당하는 가중치(Weight)와 편향(bias)의 변화가 '선형 구조'를 보이기 때문이다.  

> 조금 더 명확하게 이해하기 위해 하나의 예를 들겠다. 우리 앞에 모눈종이가 있다고 하자. 그 모눈 종이는 1을 나타내는 세모와 0을 나타내는 동그라미로 값을 표현하는데, X와 Y의 좌표 중 0과 1을 지니는 곳에만 표시할 수 있다면 어떻게 그려질까?  
>
> 바로 (0, 0), (0, 1), (1, 0), (1, 1)에만 그릴 수 있는 것이다. 이제 우리는 여기에 AND, NAND, OR, XOR을 진행했을 때의 값을 동그라미와 세모로 그려보고 오직 직선을 이용해 동그라미와 세모의 분포를 구분해보자(직선이기만 하면 어떤 방식으로 그리든 상관없다).  
>
> 어떤가? AND, NAND, OR는 직선으로 구분할 수 있지만, XOR는 곡선으로 그리거나 원형으로 그릴 수밖에 없지 않은가? 바로 이러한 이유로 하나의 퍼셉트론만을 이용해서는 XOR을 표현할 수 없다.  
  
이러한 문제를 해결하기 위해 다층(Multi-Layer) 퍼셉트론이 등장했다. 말이 어려워서 그렇지 그냥 여러 개의 논리 게이트를 섞는 방법을 말한다.  

> 이제 XOR를 나타내보자. XOR을 나타내기 위해서는 AND, NAND, OR 하나씩 필요하다. 우리 상상의 모눈종이에 이를 그려보도록 하자.  
  
##### Nueral Network  

Nueral Network, 즉 신경망은 퍼셉트론의 또다른 문제를 해결하기 위해 등장했다. 우리는 이제 문제가 끝난줄 알았다. 다층 퍼셉트론을 이용하면 XOR 문제까지 해결할 수 있기 때문에 사실상 논리적인 모든 문제는 풀 수 있을 것 같았는데 무엇이 문제일까?  

바로 가중치를 설정하는 작업은 사람의 손을 거쳐야 한다는 것이다. 앞서 말했던 것처럼 다층 퍼셉트론을 이용할 경우 여러 층에서 가중치를 필요로 한다.  

일반적으로 우리가 꿈꾸는 이상적인 함수는 입력 값만 주면 아무런 조치를 취하지 않아도 원하는 출력 값을 내놓는 것이다. 그런데 원하는 출력 값을 위해서는 앞서 언급한 것처럼 가중치와 편향의 수동적인 변화가 필요하다.  

> 지금까지 살펴본 내용은 무엇일까라는 의문과 허탈감에 휩쌓일지도 모른다. 하지만, 신경망을 통해 이러한 문제를 해결할 수 있고 어떤 식으로 이를 해결하는지를 살펴본다면 모든 의문과 허탈감은 사라지고 배우는 즐거움에 휩쌓이게 될 것이다.  
>
> 만약 앞선 학습이 헛되었다고 느낀다면 신경망과 퍼셉트론은 뉴런이 연결되는 방식이 같다는(출력이 입력으로 들어가는) 사실을 염두해두자.  
  
신경망은 크게 입력층, 은닉층, 출력층으로 나뉜다. 입력층은 0층, 은닉층은 1층, 출력층은 2층으로 표현할 수 있는데 각 층은 이름 그대로 입력을 받는 층과 숨겨진(사람이 식별할 수 없는) 층, 출력을 하는 층으로 설명할 수 있다.  

> 여기서는 이러한 신경망을 이야기할 때 가중치를 갖는 층은 2개의 층(은닉층, 출력층)이기 때문에 '2층 신경망'이라고 이야기한다.
>
> 그리고 층이 늘어난다는 의미는 은닉층을 늘린다는 의미와 같다. 입력층과 출력층은 각 하나씩 존재한다.
  
잠시 퍼셉트론으로 돌아가자. 퍼셉트론에서는 입력값 x, y와 더불어 가중치 w(weight)와 편향 b(bias)가 존재한다고 했다. 이것을 하나의 식으로 나타내면 다음과 같을 것이다.  

```
----- 0 (b + w1 * x + w2 * y <= 0)
z = {
----- 1 (b + w1 * x + w2 * y > 0)
```
  
이 식을 풀어서 설명하면, "`b + w1 * x + w2 * y`의 값이 0보다 작거나 같으면 출력(z)을 0으로 하고, 크면 출력을 1로 하라."는 말이다.  

그리고 이 식을 조금 더 간결한 형태로 작성하면 다음과 같이 작성할 수 있다.  

```
z = h(b + w1 * x + w2 * y)

-------- 0 (k <= 0)
h(k) = {
-------- 1 (k > 0)
```
  
우리는 이 식을 통해 입력 신호의 총합이 h(k)라는 함수를 거쳐 변환되어, 그 변환된 값이 z의 출력이 된다는 것을 설명할 수 있다.  

여기서의 h(k)를 '활성화(activation) 함수'라고 한다. 이는 입력 신호의 총합이 활성화를 일으키는지(0인지 1인지)를 정하는 역할을 하는 함수이다.  

> 여기서 뜬금없이 활성화 함수를 등장시킨 이유는 이 활성화 함수가 퍼셉트론을 신경망으로 이끌기 위한, 진화시키기 위한 키(key)이기 때문이다.  
>
> 그리고 일반적으로 단순 퍼셉트론은 단층 네트워크에서 계단 함수(임계값을 경계로 출력이 바뀌는 함수)를 활성화 함수로 사용한 모델을 말하고, 다층 퍼셉트론은 신경망에서 시그모이드 함수 등의 매끄러운 활성화 함수를 사용하는 네트워크를 말한다.
  
간단하게 임계값을 경계로 출력이 바뀌는 계단 함수에 대해 살펴보자. 퍼셉트론에서는 계단 함수를 활성화 함수로 사용한다. 또한, 추가적으로 계단 함수 이외의 활성화 함수도 존재한다.  

가령 대표적인 예로 '시그모이드' 함수가 있다. 이 함수는 신경망에서 자주 이용하는 활성화 함수로 다음과 같이 나타낼 수 있다.  

```
h(z) = 1 / 1 + exp(-x)  
```
  
여기서 exp(-x)는 생소할 수도 있다. 이 의미는 자연상수 e가 -x를 지수로 갖는 형태를 함수로 표현한 것이다(자연상수 e는 2.7182...의 값을 갖는 실수이다).  

이렇게 설명하니 다소 어려운 느낌이 들지만, 그럼에도 시그모이드는 하나의 함수에 불과하다. 함수는 입력을 주면 출력을 돌려주는 변환기일 뿐이기 때문에 그 구체적인 내용을 몰라도 그저 신경망에서 시그모이드를 사용한다는 것이 중요한 내용이다.  

> 사실 퍼셉트론과 신경망의 주된 차이는 이 활성화 함수의 종류이다. 실습을 통해 시그모이드 함수와 계단 함수의 그래프를 비교해보자.  
>
> 이 둘의 가장 큰 차이는 '매끄러움'에 있다. 시그모이드 함수는 부드러운 곡선이며 입력에 다라 출력이 연속적으로 변한다. 반면, 계단 함수는 0을 경계로 갑작스럽게 출력이 변화한다.  
  
계단함수와 시그모이드의 가장 중요한 차이점은 계단 함수를 사용하는 퍼셉트론의 뉴론은 0과 1이 흐른다면 시그모이드 함수를 사용하는 신경망은 실수(0.xxx, 0.yyy 등)이 흐른다는 것이다.  

또한, 이 둘의 가장 큰 공통점이자 활성화 함수의 가장 큰 특징은 입력이 중요하면 큰 값을 출력하고, 입력이 중요하지 않으면 작은 값을 출력한다는 것이다. 이는 앞으로 살펴볼 '학습한 것과 비슷한 데이터인지를 구분'하는 기준이 되기 때문에 주요하게 다룰 필요가 있다.  

> 신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다(계단 함수와 시그모이드 함수는 모두 비선형 함수이다). 그 이유는 층을 깊게 하는 의미가 사라지기 때문이다.  
>
> 층을 깊게 한다는 것은 보다 변칙적인 문제를 해결하기 위함인데 선형 함수에서는 아무리 층을 깊게 해도 그것을 은닉층이 없는 네트워크로 구성할 수 있다.
  
그리고 요즘에는 ReLU(렐루) 함수를 주로 이용한다. ReLU의 그래프는 실습을 통해 살펴볼 수 있는데, 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하이면 0을 출력하는 함수이다.  

이제 본격적으로 신경망을 구성해보도록 하자. 신경망을 구성하기 전에 유의할 점이 있다. 그것은 신경망을 구성하는데 있어서 입력값 x와 가중치 w가 내적을 구할 수 있는 형태(2x2 * 2x2, 2x3 * 3x2, ...)여야 한다.  

쉽게 설명하자면 입력이 2개 존재할 경우 입력된 2개의 값이 다음의 모든 노드로 전달되어야 한다는 의미이다. 이 경우 2(개의 입력)x2(곳으로 전달) * 2(개의 값을 받아)x2(개의 출력을 전달)한다는 의미를 지닌다.

자세한 내용은 실습을 통해 이해하도록 하자.  


#### 참고자료  

[밑바닥부터 시작하는 딥러닝 - 사이토 고키](http://www.hanbit.co.kr/store/books/look.php?p_code=B8475831198)  


