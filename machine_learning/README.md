# Round 5 ML(Machine Learning) basic

##### Machine Learning으로 배우는 인공지능의 세계

## Round 5-1 AI? ML? DL?

<div align=center>

![](assets/aimldl.png)

</div>

#### 인공지능 기술의 탄생 및 성장

인공 지능이라는 개념은 1956년 미국 다트머스 대학에 있던 존 매카시 교수가 개최한 다트머스 회의에서 처음 등장했으며, 최근 몇 년 사이 폭발적으로 성장하고 있는 중입니다. 특히 2015년 이후 빠르고, 강력한 병렬 처리 성능을 제공하는 GPU의 도입으로 그 속도는 지금도 가속화되고 있습니다.

> 또한 갈수록 폭발적으로 늘어나고 있는 저장 용량과 이미지, 텍스트, 매핑 데이터 등 모든 영역의 데이터가 범람하게 된 '빅데이터' 시대의 도래도 이러한 성장세에 큰 영향을 미쳤습니다.

#### AI(Artificial Intelligence)?

1956년 당시 인공 지능의 선구자들이 꿈꾼 것은 최종적으로 인간의 지능과 유사한 특성을 가진 복잡한 컴퓨터를 제작하는 것이었습니다.

이처럼 인간의 감각, 사고력을 지닌 채 인간처럼 생각하는 인공 지능을 **'일반 AI(General AI)'**라고 하지만, 현재의 기술 발전 수준에서 만들 수 있는 인공지능은 **'좁은 AI(Narrow AI)'**의 개념에 포함되어 아직은 일반 AI를 구현할 수준까지 도달하지 못했습니다.

> 좁은 AI는 소셜 미디어의 이미지 분류 서비스나 얼굴 인식 기능 등과 같이 특정 작업을 인간 이상의 능력으로 해낼 수 있는 것이 특징입니다.

#### ML(Machine Learing)?

**"머신 러닝은 메일함의 스팸을 자동으로 걸러주는 역할을 합니다."**

한편, 머신 러닝은 기본적으로 알고리즘을 이용해 데이터를 분석하고, 분석을 통해 학습하며, 학습한 내용을 기반으로 판단이나 예측을 합니다. 따라서 궁극적으로는 의사 결정 기준에 대한 구체적인 지침을 소프트웨어에 직접 코딩해 넣는 것이 아닌, 대량의 데이터와 알고리즘을 통해 컴퓨터 그 자체를 **'학습'**시켜 작업 수행 방법을 익히는 것을 목표로 한답니다.

머신 러닝은 초기 인공 지능 연구자들이 직접 제창한 개념에서 나온 것이며, 알고리즘 방식에는 의사 결정 트리 학습, 귀납 논리 프로그래밍, 클러스터링, 강화 학습, 베이지안(Bayesian) 네트워크 등이 포함됩니다. 그러나 이 중 어느 것도 최종 목표라 할 수 있는 일반 AI를 달성하진 못했으며, 초기의 머신 러닝 접근 방식으로는 좁은 AI조차 완성하기 어려운 경우도 많았던 것이 사실이죠.

이에 반해 현재 머신 러닝은 컴퓨터 비전 등의 분야에서 큰 성과를 이뤄내고 있으나, 구체적인 지침이 아니더라도 인공 지능을 구현하는 과정 전반에 일정량의 코딩 작업이 수반된다는 한계점에 봉착하기도 했습니다.

가령 머신 러닝 시스템을 기반으로 정지 표지판의 이미지를 인식할 경우, 개발자는 물체의 시작과 끝 부분을 프로그램으로 식별하는 경계 감지 필터, 물체의 면을 확인하는 형상 감지, 'S-T-O-P'와 같은 문자를 인식하는 분류기 등을 직접 코딩으로 제작해야 합니다.

> 이처럼 머신 러닝은 '코딩'된 분류기로부터 이미지를 인식하고, 알고리즘을 통해 정지 표지판을 '학습'하는 방식으로 작동된답니다.
>
> 머신 러닝의 이미지 인식률은 상용화하기에 충분한 성능을 구현하지만, 안개가 끼거나 나무에 가려서 표지판이 잘 보이지 않는 특정 상황에서는 이미지 인식률이 떨어지기도 한답니다. 최근까지 컴퓨터 비전과 이미지 인식이 인간의 수준으로 올라오지 못한 이유는 이 같은 인식률 문제와 잦은 오류 때문이죠.

#### DL(Deep Learing)?

초기 머신 러닝 연구자들이 만들어 낸 또 다른 알고리즘인 **'인공 신경망(artificial neural network)'**에 영감을 준 것은 인간의 뇌가 지닌 생물학적 특성, 특히 뉴런의 연결 구조였습니다. 그러나 물리적으로 근접한 어떤 뉴런이든 상호 연결이 가능한 뇌와는 달리, 인공 신경망은 레이어 연결 및 데이터 전파 방향이 일정합니다.

> 예를 들어, 이미지를 수많은 타일로 잘라 신경망의 첫 번째 레이어에 입력하면, 그 뉴런들은 데이터를 다음 레이어로 전달하는 과정을 마지막 레이어에서 최종 출력이 생성될 때까지 반복합니다. 그리고 각 뉴런에는 수행하는 작업을 기준으로 입력의 정확도를 나타내는 가중치가 할당되며, 그 후 가중치를 모두 합산해 최종 출력이 결정됩니다.
>
> 정지 표지판의 경우, 팔각형 모양, 붉은 색상, 표시 문자, 크기, 움직임 여부 등 그 이미지의 특성이 잘게 잘려 뉴런에서 '검사'되며, 신경망의 임무는 이것이 정지 표지판인지 여부를 식별하는 것입니다. 여기서는 충분한 데이터를 바탕으로 가중치에 따라 결과를 예측하는 **'확률 벡터(probability vector)'**가 활용되죠.

**Deep Learning**은 인공신경망에서 발전한 형태의 인공 지능으로, 뇌의 뉴런과 유사한 정보 입출력 계층을 활용해 데이터를 학습합니다. 그러나 기본적인 신경망조차 굉장한 양의 연산을 필요로 하는 탓에 딥 러닝의 상용화는 초기부터 난관에 부딪혔죠. 그럼에도 토론토대의 제프리 힌튼(Geoffrey Hinton) 교수 연구팀과 같은 일부 기관에서는 연구를 지속했고, 슈퍼컴퓨터를 기반으로 딥 러닝 개념을 증명하는 알고리즘을 병렬화하는데 성공했습니다. 그리고 병렬 연산에 최적화된 GPU의 등장은 신경망의 연산 속도를 획기적으로 가속하며 진정한 딥 러닝 기반 인공 지능의 등장을 불러왔죠.

> 신경망 네트워크는 '학습' 과정에서 수많은 오답을 낼 가능성이 큽니다. 정지 표지판의 예로 돌아가서, 기상 상태, 밤낮의 변화에 관계 없이 항상 정답을 낼 수 있을 정도로 정밀하게 뉴런 입력의 가중치를 조정하려면 수백, 수천, 어쩌면 수백만 개의 이미지를 학습해야 할지도 모르죠. 이 정도 수준의 정확도에 이르러서야 신경망이 정지 표지판을 제대로 학습했다고 볼 수 있습니다.

## Round 5-2 머신러닝의 개념과 용어


## Round 5-3 Learning to 'Machine Learing'

##### Learning environment


## Round 5-x Begining to Deep Learning

# 딥러닝의 시작

## 인류의 간절한 염원, 인공지능

인공지능을 만들기 위해 다양한 분야에서 수많은 이론이 탄생하게 되었다. 그 중 인간의 뇌를 모방하자는 이론이 바로 'Deep Learning'이다. 인간의 뇌는 다양한 동작을 하게 되는데, 이러한 다양한 동작은 가령 상상, 생각, 무의식 등이다.  

이를 신기하게 여긴 학자들은 다양한 동작을 가능하게 하는 것이 뇌 속의 '뉴런'이 모여 서로 작용하기 때문이라는 것을 밝혀낸다.  

이러한 뉴런은 미량의 전기신호로 정보를 전달하는데, 여기서 뉴런에 입력된 정보를 `x`, 정보를 구분하기 위한 신호의 가중치(weight)를 `w`라고 할 때, 뉴런의 각기 다른 촉수에서 들어온 '`x`와 `w`의 곱'들의 합을 정보의 집합(I)으로 간주할 수 있다.    


![](https://github.com/rjs1197/ml_study/blob/master/assets/1st_week_1.png)

```
입력된 값(x) * 신호의 가중치(w) 들의 합(sigma Σ) = 정보의 집합(I)
```

이렇게 만들어진 하나의 정보는 다음 뉴런으로 전달되기 위해 방향성을 조절하기 위한 값(bios)과 더해지고, 이 값을 활용해 어떠한 동작(function)을 수행하는 과정을 거친다. 여기서 수행되는 과정은 어떤 값 n에 대해 if n > I + b라면 다음 뉴런으로 전달하는 것을 말한다.  

이제 우리는 다음과 같이 하나의 수식을 통해 이 동작을 정의할 수 있다.  

![](https://github.com/rjs1197/ml_study/blob/master/assets/1st_week_2.png)

```
동작(func){정보의 집합(I) + 바이어스(b)} = 가공된 정보(Y)
```

위 과정을 거쳐 하나의 뉴런에서 또다른 뉴런으로 가공된 정보가 이동여부를 결정하게 되며, 이러한 정보 이동의 집합을 'Neural Network'라고 부르게 되었다. 이 과정을 유심히 본 공학자들은 '인공적인 신경망'을 떠올리게 되었다.  

```
이런 수식을 가졌다면 우리도 만들 수 있겠는데?
```

그렇게 탄생한 것이 'Artificial Neural Network(인공신경망)이다. 이러한 이론이 제기된 초기에는 아래와 같이 하드웨어로 구현할 수밖에 없었고, 그들은 이를 '인공지능'이라고 불렀다.  

당시에는 인공지능, 즉 생각하는 기계를 만들기 위해서는 AND/OR의 연산을 예측해야 한다고 생각했고, 그들의 기계는 이를 당연하게 예측했다. 그러나 여기에도 문제가 있었는데, 완전한 인공지능을 위해서는 XOR 연산까지도 예측해야 한다는 것이었다.    

XOR 연산의 경우에는 신경망 학습을 이용한 예측에서 50%의 예측률을 보였고, 신경망 학습을 이용해 완전하게 XOR을 예측하는 것은 불가능하다고 단정짓는다.  

이 불가능을 단정지어버린 사건은 짧게는 15년, 길게는 20년 딥러닝의 역사를 늦춘 계기가 된다. 이후 1986년 Hinton에 의해 해결되기 전까지 '딥러닝'은 '불가능한 기술'이라는 꼬리표를 달았어야 했다(1974년에 이미 해결 방법이 발견되었지만, 당시 학계 분위기로 인해 사장되었다. 시간이 흘러 결국 Hinton이 이를 재발견하게 되었다).  

![](https://github.com/rjs1197/ml_study/blob/master/assets/1st_week_3.png)

이 논문은 학습된 신경망의 출력이 우리가 원하는 값과는 다를 경우 정보를 반대로 전달(backpropagation)하여 뉴런 각각을 변경시켜 결과를 바꾸는 이론이다. 이로인해 XOR은 물론 더 복잡한 형태의 예측이 가능해졌다.  

또 한편으로는 CNNs이라고 불리는 'Convolutional Neural Networks'이 LeCun에 의해 발표되었다.  

![](https://github.com/rjs1197/ml_study/blob/master/assets/1st_week_4.png)

이 논문은 고양이의 뇌를 관찰하며 발견한 신경망의 작용에 대해 설명한다. 가령 위 그림의 문자를 이해하는데, 고양이의 뇌는 하나의 문자로 인식하는 것이 아닌 여러 문자의 조합으로 인식했다는 것이다.  

조금 깊이 설명하면 어떤 뉴런은 직선을, 어떤 뉴런은 원을 인식하는 등 각기 다른 작용을 하는 뉴런이 집합을 이뤄 하나의 정보를 조합했다는 것을 말한다.  

이러한 작용을 활용하여 다음과 같이 하나의 그림을 잘게 쪼개고 나중에 합치는 방식으로 네트워크를 구성하는 방식을 만들었다(알파고도 이 방식으로 학습되었다).  


![](https://github.com/rjs1197/ml_study/blob/master/assets/1st_week_5.png)

이후 'Neural Network'는 자율주행차를 만드는 등 활발하게 활용되는가 싶더니 큰 위기에 봉착하게 된다. Backpropagation을 사용한 알고리즘은 소량의 레이어를 지닌 문제에서는 잘 학습되지만, 실제로 필요한 10여개 이상의 레이어를 가진 문제에서는 제대로 동작하지 않았다.  

10여개 이상의 레이어를 학습시켜야 하는데, backpropagation 알고리즘이 발생한 에러를 뒤로 보낼 때 뒤로 갈수록 에러가 지닌 의미가 거의 전달되지 않아 성능이 떨어지게 되었다.  

backpropagation을 중심으로 한 'Neural Network' 알고리즘의 성능은 당시 학계에서 언급되던 다른 알고리즘에 비해 현저하게 떨어졌고, 심지어 CNNs를 만든 LeCun조차 다른 알고리즘의 성능을 인정하였고, 다시 한 번 'Neural Network'는 침체기에 빠진다.  


![](https://github.com/rjs1197/ml_study/blob/master/assets/1st_week_6.png)

침체기는 10년 정도의 시간이 지난 후 2006년에 CIFAR라는 단체에서 연구를 지속하던 Hinton과 Bengio에 의해 벗어나게 된다. 우리는 지금까지 굉장히 deep한(레이어가 많은) 신경망은 학습이 불가능하다고 이야기했다.  

이러한 상황에서 Hinton과 Bengio는 2006년 논문에서 그것은 불가능한 게 아니라 초기값을 잘 조정하면 학습이 가능하다고 말했다. 또한, 2007년 논문에서는 2006년 논문을 기반으로 deep한 신경망을 구축하면 굉장히 복잡한 문제를 해결할 수 있다는 것을 증명했다. 학계가 이를 주목한 이유는 또 있었다.  

![](https://github.com/rjs1197/ml_study/blob/master/assets/1st_week_7.png)

그 이유는 'Neural Network'라는 홀대받던 이름을 당시 학계 분위기를 고려해 'Deep Learning', 'Deep Network'라고 바꿔 발표했기 때문이다. 결국 학계는 다시 신경망에 관심을 갖게 되었고, 활발하게 연구하기 시작했다.  

이 논문을 시작으로 딥러닝은 엄청난 주목을 받게 된 계기가 있었는데, 'IMAGENET'이라는 대회에서 2010년 30%의 에러율을 보였던 이 기술이 2012년 15%의 에러율을 보였고, 2015년 3%의 에러율을 보인 사건이 그 계기이다.  

크게 주목을 받게 된 딥러닝은 단순히 그림을 인식하는 것을 넘어 그림을 설명하는 단계까지 발전하였고, 자연어로 된 프로그램 계획서의 구문을 보고 필요한 API를 자동으로 찾아주거나, 주변 소음이 많은 곳에서 90%까지 소음을 차단해주는 기술 등으로 발전하게 되었다.  

![](https://github.com/rjs1197/ml_study/blob/master/assets/1st_week_8.png)

물론 주변에서도 찾아볼 수 있는데, 알파고나 유튜브의 자동 자막 생성 시스템, 페이스북의 feed 추천, 구글의 검색엔진 등이 그것이다.  

### 참고자료

[nvidia 블로그 - 인공 지능과 머신 러닝, 딥 러닝의 차이점을 알아보자][1]  
[모두를 위한 머신러닝/딥러닝][2]  
[앤드류 응(Andrew Ng) - Machine Learing][3]  
[다프네 콜러(Darphne Koller) - Probabilistic Graphical Model][4]  
[제프리 힌튼(Jeofrrey Hinton) - Neural Networks for Machine Learning][5]  
[http://aikorea.org/cs231n/convolutional-networks/][6]  
[ec 08-2: 딥러닝의 기본 개념2: Back-propagation 과 2006/2007 '딥'의 출현][7]  
[lec 08-1: 딥러닝의 기본 개념: 시작과 XOR 문제][8]  

[1]: http://blogs.nvidia.co.kr/2016/08/03/difference_ai_learning_machinelearning/  
[2]: https://hunkim.github.io/ml/  
[3]: https://www.coursera.org/learn/machine-learning  
[4]: https://www.coursera.org/learn/probabilistic-graphical-models  
[5]: https://www.coursera.org/learn/neural-networks  
[6]: http://aikorea.org/cs231n/convolutional-networks/  
[7]: https://www.youtube.com/watch?v=AByVbUX1PUI  
[8]: https://www.youtube.com/watch?v=n7DNueHGkqE&feature=youtu.be  
